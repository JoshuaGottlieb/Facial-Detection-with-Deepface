{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19be49e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 13:06:04.108635: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-23 13:06:04.154523: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-23 13:06:04.155783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-23 13:06:04.996624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, LocallyConnected2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491aa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dbe3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deepface_model(input_shape = (200, 200, 3)):\n",
    "    deepface = keras.Sequential(name = 'DeepFace')\n",
    "    deepface.add(Convolution2D(32, (11, 11), activation = relu, name = 'C1', input_shape = input_shape))\n",
    "    deepface.add(MaxPooling2D(pool_size = 3, strides = 2, padding = 'same', name = 'M2'))\n",
    "    deepface.add(Convolution2D(16, (9, 9), activation = relu, name = 'C3'))\n",
    "    deepface.add(LocallyConnected2D(16, (9, 9), activation = relu, name = 'L4'))\n",
    "    deepface.add(LocallyConnected2D(16, (7, 7), strides = 2, activation = relu, name = 'L5'))\n",
    "    deepface.add(LocallyConnected2D(16, (5, 5), activation = relu, name = 'L6'))\n",
    "    deepface.add(Flatten(name = 'F0'))\n",
    "    deepface.add(Dense(4096, activation = relu, name = 'F7'))\n",
    "    deepface.add(Dropout(rate = 0.5, name = 'D0'))\n",
    "    deepface.add(Dense(4038, activation = softmax, name = 'F8'))\n",
    "    \n",
    "    return deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aeffd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, **kwargs):\n",
    "    model.compile(\n",
    "        optimizer = kwargs['optimizer'],\n",
    "        loss = kwargs['loss'],\n",
    "        metrics = kwargs['metrics']\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1841c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(**kwargs):\n",
    "    lr_reducer = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = kwargs['lr_factor'],\n",
    "        patience = kwargs['lr_patience'],\n",
    "        verbose = kwargs['lr_verbose'],\n",
    "        mode = 'auto',\n",
    "        min_delta = kwargs['lr_delta'],\n",
    "        cooldown = kwargs['lr_cooldown'],\n",
    "        min_lr = kwargs['lr_min']\n",
    "    )\n",
    "    early_stopper = keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = kwargs['stopper_delta'],\n",
    "        patience = kwargs['stopper_patience'],\n",
    "        verbose = kwargs['stopper_verbose'],\n",
    "        mode = 'auto',\n",
    "        baseline = None,\n",
    "        restore_best_weights = False,\n",
    "        start_from_epoch = kwargs['stopper_epoch']\n",
    "    )\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath = kwargs['checkpoint_path'],\n",
    "        monitor = 'val_loss',\n",
    "        verbose = kwargs['checkpoint_verbose'],\n",
    "        save_best_only = kwargs['checkpoint_best'],\n",
    "        save_weights_only = kwargs['checkpoint_weights'],\n",
    "        mode = 'auto',\n",
    "        save_freq = kwargs['checkpoint_freq'],\n",
    "        initial_value_thresold = kwargs['checkpoint_threshold']\n",
    "    )\n",
    "    \n",
    "    return [lr_reducer, early_stopper, checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7865dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, **kwargs):\n",
    "    history = model.fit(\n",
    "        x = kwargs['x_train'],\n",
    "        y = kwargs['y_train'],\n",
    "        epochs = kwargs['epochs'],\n",
    "        batch_size = kwargs['batch_size'],\n",
    "        callbacks = kwargs['callbacks'],\n",
    "        validation_data = kwargs['val_data'],\n",
    "        initial_epoch = kwargs['initial_epoch'],\n",
    "        steps_per_epoch = kwargs['steps_per_epoch'],\n",
    "        validation_steps = kwargs['validation_steps'],\n",
    "        verbose = 1\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b39b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_mappings(train_dir):\n",
    "    labels = ([' '.join(x.split('_')[0:-1]) for x in os.listdir(train_dir)])\n",
    "    seen = set()\n",
    "    unique_labels = [x for x in labels if not (x in seen or seen.add(x))]\n",
    "    numeric_labels = [i for i, k in enumerate(unique_labels)]\n",
    "    \n",
    "#     table = tf.lookup.StaticHashTable(\n",
    "#         tf.lookup.KeyValueTensorIniitalizer(tf.constant(unique_labels), tf.constant(numeric_labels)),\n",
    "#         default_value = -1\n",
    "#     )\n",
    "    \n",
    "#     return table\n",
    "    return {unique_labels[i]:numeric_labels[i] for i in range(len(unique_labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6c8662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_label_mappings(mappings, path = './data/preprocessed/label_mappings.csv'):\n",
    "    with open(path, 'w') as f:\n",
    "        for i, key in enumerate(mappings.keys()):\n",
    "            if i != len(mappings.keys()):\n",
    "                line_sep = '\\n'\n",
    "            else:\n",
    "                line_sep = ''\n",
    "            f.write(f'{key},{mappings[key]}{line_sep}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca56c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label(label, label_map, num_labels = 4038):\n",
    "    numeric_label = label_map.lookup(label)\n",
    "#     numeric_label = label_map[label]\n",
    "    return tf.one_hot(indices = numeric_label, depth = num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f617262",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def parse_image(file_path, label_map, num_labels = 4038):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "#     label = tf.py_function(\n",
    "#         map_label, inp = [' '.join(parts[-1].numpy().decode('utf-8').split(os.path.sep)[-1].split('_')[0:-1]),\n",
    "#                            label_map, num_labels], Tout = [tf.float32])\n",
    "    \n",
    "#     label = map_label(tf.strings.join((parts[-1].numpy().decode('utf-8').split(os.path.sep)[-1].split('_')[0:-1])),\n",
    "#                            label_map, num_labels = num_labels)\n",
    "    \n",
    "    label = parts[-1]\n",
    "    label = tf.strings.split(label, os.path.sep)[-1]\n",
    "    label = tf.strings.split(label, '_')[0:-1]\n",
    "#     print(label)\n",
    "    label = tf.strings.reduce_join(label, separator = ' ')\n",
    "    label = map_label(label, label_map, num_labels = num_labels)\n",
    "    \n",
    "#     label = map_label(tf.strings.join((parts[-1].numpy().decode('utf-8').split(os.path.sep)[-1].split('_')[0:-1])),\n",
    "#                            label_map, num_labels = num_labels)\n",
    "    \n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deca43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_dir = './data/preprocessed/padded/train'\n",
    "validation_image_dir = './data/preprocessed/padded/validation'\n",
    "\n",
    "if not os.path.exists(validation_image_dir):\n",
    "    os.makedirs(validation_image_dir)\n",
    "    \n",
    "# If validation split has not occurred, create a validation split using labels with at least 2 images\n",
    "# This ensure that the validation set data has representatives in the training set data with the same label\n",
    "# In the LFW set, the training image set has 9525 images, and the number of labels with 2 images is 1184\n",
    "# Thus, the validation holdout is 1184 / 9525 ~ 12.4% holdout\n",
    "if len(os.listdir(training_image_dir)) == 9525 and len(os.listdir(validation_image_dir)) != 1184:\n",
    "    validation_image_path_leafs = [x for x in os.listdir(training_image_dir) if '_0002' in x]\n",
    "    \n",
    "    for leaf in validation_image_path_leafs:\n",
    "        shutil.move(os.path.join(training_image_dir, leaf), os.path.join(validation_image_dir, leaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1bae196",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_label_mappings(training_image_dir)\n",
    "write_label_mappings(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e689ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.TextFileInitializer(\n",
    "        './data/preprocessed/label_mappings.csv',\n",
    "        key_dtype = tf.string, key_index = 0,\n",
    "        value_dtype = tf.int64, value_index = 1,\n",
    "        delimiter = ','\n",
    "    ), default_value = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4263e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.list_files(os.path.join(training_image_dir, '*.jpg'))\n",
    "train_ds = train_ds.map(lambda x: parse_image(x, label_map))\n",
    "train_ds = train_ds.shuffle(8341).batch(1024).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7ca69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.list_files(os.path.join(validation_image_dir, '*.jpg'))\n",
    "val_ds = val_ds.map(lambda x: parse_image(x, label_map))\n",
    "val_ds = val_ds.shuffle(1184).batch(1024).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50749d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f98a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepface = get_deepface_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa3cef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_params = {\n",
    "    'optimizer': keras.optimizers.SGD(learning_rate = 0.01 * (1024 / 128), momentum = 0.9),\n",
    "    'loss': keras.losses.categorical_crossentropy,\n",
    "    'metrics': ['accuracy']\n",
    "}\n",
    "compile_model(deepface, **compile_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eea0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_params = {\n",
    "    'lr_factor': 0.1,\n",
    "    'lr_patience': 1,\n",
    "    'lr_verbose': 1,\n",
    "    'lr_delta': 0.0001,\n",
    "    'lr_cooldown': 0,\n",
    "    'lr_min': 0.0001,\n",
    "    'stopper_delta': 0.001,\n",
    "    'stopper_patience': 1,\n",
    "    'stopper_verbose': 1,\n",
    "    'stopper_epoch': 0,\n",
    "    'checkpoint_path': './models/ckpt/deepface_{epoch}',\n",
    "    'checkpoint_verbose': 1,\n",
    "    'checkpoint_best': False,\n",
    "    'checkpoint_weights': False,\n",
    "    'checkpoint_freq': 'epoch',\n",
    "    'checkpoint_threshold': None\n",
    "}\n",
    "callbacks = create_callbacks(**callback_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2983198",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'x_train': train_ds,\n",
    "    'y_train': None,\n",
    "    'epochs': 1,\n",
    "    'batch_size': 1024,\n",
    "    'callbacks': callbacks,\n",
    "    'val_data': val_ds,\n",
    "    'initial_epoch': 0,\n",
    "    'steps_per_epoch': (8341 // 1024) + 1,\n",
    "    'validation_steps': (1184 // 1024) + 1\n",
    "}\n",
    "\n",
    "train_history = train_model(deepface, **training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec103d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
